{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoImageProcessor, SwinForMaskedImageModeling\n",
    "from PIL import Image\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load and convert original and masked images to RGB\n",
    "original_image_path = \"p1.png\"\n",
    "masked_image_path = \"masked_p1.png\"  # Path to the masked image\n",
    "\n",
    "original_image = Image.open(original_image_path).convert(\"RGB\")\n",
    "masked_image = Image.open(masked_image_path).convert(\"RGB\")\n",
    "\n",
    "# Initialize processor and model, either load pre-trained from Hugging Face or local path\n",
    "# Save processor and model after initialization\n",
    "processor_path = \"processor\"\n",
    "model_path = \"model\"\n",
    "\n",
    "# Uncomment the following lines if you haven't already saved the model and processor\n",
    "# image_processor = AutoImageProcessor.from_pretrained(\"microsoft/swin-base-simmim-window6-192\")\n",
    "# model = SwinForMaskedImageModeling.from_pretrained(\"microsoft/swin-base-simmim-window6-192\")\n",
    "# model.save_pretrained(model_path)\n",
    "# image_processor.save_pretrained(processor_path)\n",
    "\n",
    "# Load processor and model from saved paths\n",
    "image_processor = AutoImageProcessor.from_pretrained(processor_path)\n",
    "model = SwinForMaskedImageModeling.from_pretrained(model_path)\n",
    "\n",
    "# Process original and masked images\n",
    "pixel_values_original = image_processor(images=original_image, return_tensors=\"pt\").pixel_values\n",
    "pixel_values_masked = image_processor(images=masked_image, return_tensors=\"pt\").pixel_values\n",
    "\n",
    "# Create a mask for the masked image (Assuming the mask is known or can be derived)\n",
    "num_patches = (model.config.image_size // model.config.patch_size) ** 2\n",
    "# Here, you should load or define the mask that corresponds to the masked image\n",
    "# For demonstration, let's assume we have a similar random mask\n",
    "bool_masked_pos = torch.randint(low=0, high=2, size=(1, num_patches)).bool()\n",
    "\n",
    "# Run the model with the masked image\n",
    "outputs = model(pixel_values_masked, bool_masked_pos=bool_masked_pos)\n",
    "loss, reconstructed_pixel_values = outputs.loss, outputs.reconstruction\n",
    "\n",
    "# Print results\n",
    "print(\"Reconstructed pixel values shape:\", reconstructed_pixel_values.shape)\n",
    "print(\"Loss:\", loss.item())\n",
    "\n",
    "# Convert reconstructed pixel values to image and display\n",
    "reconstructed_image = reconstructed_pixel_values.detach().cpu().squeeze().permute(1, 2, 0).numpy()\n",
    "plt.imshow(reconstructed_image)\n",
    "plt.title(\"Reconstructed Image from Masked Input\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cogs118a",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
